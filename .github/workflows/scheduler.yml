name: Auto Crawl Party News

on:
  schedule:
    # 한국 시간 (KST = UTC+9)
    # 오전 10:30 KST = 01:30 UTC
    - cron: '30 1 * * *'
    # 오후 15:30 KST = 06:30 UTC
    - cron: '30 6 * * *'
    # 저녁 19:30 KST = 10:30 UTC
    - cron: '30 10 * * *'

  # 수동 실행 가능
  workflow_dispatch:

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt

    - name: Install Chrome and ChromeDriver
      run: |
        # Chrome 설치
        wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        echo "deb http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable

        # ChromeDriver 설치
        CHROME_VERSION=$(google-chrome --version | awk '{print $3}' | cut -d'.' -f1)
        wget -q "https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/${CHROME_VERSION}.0.6778.69/linux64/chromedriver-linux64.zip"
        unzip -q chromedriver-linux64.zip
        sudo mv chromedriver-linux64/chromedriver /usr/local/bin/
        sudo chmod +x /usr/local/bin/chromedriver

    - name: Run crawler
      env:
        NOTION_TOKEN: ${{ secrets.NOTION_TOKEN }}
        NOTION_DATABASE_ID: ${{ secrets.NOTION_DATABASE_ID }}
      run: |
        python src/main.py sample --notion

    - name: Upload logs (if any)
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: crawl-logs-${{ github.run_number }}
        path: |
          *.log
          *.html
        retention-days: 7
        if-no-files-found: ignore
